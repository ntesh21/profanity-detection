{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Basic packages\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import re\n",
    "import collections\n",
    "# import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Packages for data preparation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import stopwords\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model\n",
    "\n",
    "# Packages for modeling\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_WORDS = 10000  # Parameter indicating the number of words we'll put in the dictionary\n",
    "VAL_SIZE = 1000  # Size of the validation set\n",
    "NB_START_EPOCHS = 20  # Number of epochs we usually start to train with\n",
    "BATCH_SIZE = 512  # Size of the batches used in the mini-batch gradient descent\n",
    "MAX_LEN = 24  # Maximum number of words in a sequence\n",
    "GLOVE_DIM = 50  # Number of dimensions of the GloVe word embeddings\n",
    "INPUT_PATH = '../input'  # Path where all input files are stored\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = Path('./')\n",
    "input_path = root / 'input/' \n",
    "ouput_path = root / 'output/'\n",
    "source_path = root / 'source/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def deep_model(model, X_train, y_train, X_valid, y_valid):\n",
    "    '''\n",
    "    Function to train a multi-class model. The number of epochs and \n",
    "    batch_size are set by the constants at the top of the\n",
    "    notebook. \n",
    "    \n",
    "    Parameters:\n",
    "        model : model with the chosen architecture\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_valid : validation features\n",
    "        Y_valid : validation target\n",
    "    Output:\n",
    "        model training history\n",
    "    '''\n",
    "    model.compile(optimizer='rmsprop'\n",
    "                  , loss='categorical_crossentropy'\n",
    "                  , metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(X_train\n",
    "                       , y_train\n",
    "                       , epochs=NB_START_EPOCHS\n",
    "                       , batch_size=BATCH_SIZE\n",
    "                       , validation_data=(X_valid, y_valid)\n",
    "                       , verbose=1)\n",
    "    model.save(\"./output/model/model.h5\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_metric(history, metric_name):\n",
    "    '''\n",
    "    Function to evaluate a trained model on a chosen metric. \n",
    "    Training and validation metric are plotted in a\n",
    "    line chart for each epoch.\n",
    "    \n",
    "    Parameters:\n",
    "        history : model training history\n",
    "        metric_name : loss or accuracy\n",
    "    Output:\n",
    "        line chart with epochs of x-axis and metric on\n",
    "        y-axis\n",
    "    '''\n",
    "    metric = history.history[metric_name]\n",
    "    val_metric = history.history['val_' + metric_name]\n",
    "\n",
    "    e = range(1, NB_START_EPOCHS + 1)\n",
    "\n",
    "    plt.plot(e, metric, 'bo', label='Train ' + metric_name)\n",
    "    plt.plot(e, val_metric, 'b', label='Validation ' + metric_name)\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, X_train, y_train, X_test, y_test, epoch_stop):\n",
    "    '''\n",
    "    Function to test the model on new data after training it\n",
    "    on the full training data with the optimal number of epochs.\n",
    "    \n",
    "    Parameters:\n",
    "        model : trained model\n",
    "        X_train : training features\n",
    "        y_train : training target\n",
    "        X_test : test features\n",
    "        y_test : test target\n",
    "        epochs : optimal number of epochs\n",
    "    Output:\n",
    "        test accuracy and test loss\n",
    "    '''\n",
    "    model.fit(X_train\n",
    "              , y_train\n",
    "              , epochs=epoch_stop\n",
    "              , batch_size=BATCH_SIZE\n",
    "              , verbose=0)\n",
    "    results = model.evaluate(X_test, y_test)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(input_text):\n",
    "    '''\n",
    "    Function to remove English stopwords from a Pandas Series.\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def remove_mentions(input_text):\n",
    "    '''\n",
    "    Function to remove mentions, preceded by @, in a Pandas Series\n",
    "    \n",
    "    Parameters:\n",
    "        input_text : text to clean\n",
    "    Output:\n",
    "        cleaned Pandas Series \n",
    "    '''\n",
    "    return re.sub(r'@\\w+', '', input_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dell-3060/.local/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(input_path / 'train.csv')\n",
    "df = df.reindex(np.random.permutation(df.index))  \n",
    "df = df[['comment_text', 'toxic']]\n",
    "df.text = df.comment_text.apply(remove_stopwords).apply(remove_mentions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Train data samples: 143613\n",
      "# Test data samples: 15957\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.comment_text, df.toxic, test_size=0.1, random_state=37)\n",
    "print('# Train data samples:', X_train.shape[0])\n",
    "print('# Test data samples:', X_test.shape[0])\n",
    "assert X_train.shape[0] == y_train.shape[0]\n",
    "assert X_test.shape[0] == y_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = Tokenizer(num_words=NB_WORDS,\n",
    "               filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "               lower=True,\n",
    "               split=\" \")\n",
    "tk.fit_on_texts(X_train)\n",
    "# print(tk)\n",
    "# saving\n",
    "with open('./output/model/tk.pickle', 'wb') as handle:\n",
    "    pickle.dump(tk, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "X_train_seq = tk.texts_to_sequences(X_train)\n",
    "X_test_seq = tk.texts_to_sequences(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    143613.000000\n",
       "mean         67.866642\n",
       "std         100.360166\n",
       "min           1.000000\n",
       "25%          17.000000\n",
       "50%          36.000000\n",
       "75%          76.000000\n",
       "max        2273.000000\n",
       "Name: comment_text, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "seq_lengths = X_train.apply(lambda x: len(x.split(' ')))\n",
    "seq_lengths.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_seq_trunc = pad_sequences(X_train_seq, maxlen=MAX_LEN)\n",
    "X_test_seq_trunc = pad_sequences(X_test_seq, maxlen=MAX_LEN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0, 7160,   82,   51,  138,   79,    6,  469,\n",
       "         38,  130], dtype=int32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_trunc[10]  # Example of padded sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y_train_le = le.fit_transform(y_train)\n",
    "y_test_le = le.transform(y_test)\n",
    "y_train_oh = to_categorical(y_train_le)\n",
    "y_test_oh = to_categorical(y_test_le)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of validation set: (14362, 24)\n"
     ]
    }
   ],
   "source": [
    "X_train_emb, X_valid_emb, y_train_emb, y_valid_emb = train_test_split(X_train_seq_trunc, y_train_oh, test_size=0.1, random_state=37)\n",
    "\n",
    "assert X_valid_emb.shape[0] == y_valid_emb.shape[0]\n",
    "assert X_train_emb.shape[0] == y_train_emb.shape[0]\n",
    "\n",
    "print('Shape of validation set:',X_valid_emb.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_file = 'glove.twitter.27B.25d.txt'\n",
    "glove_dir = 'glove/'\n",
    "emb_dict = {}\n",
    "glove = open(input_path / glove_dir / glove_file)\n",
    "for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    vector = np.asarray(values[1:], dtype='float32')\n",
    "    emb_dict[word] = vector\n",
    "glove.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the word fuck in the dictionary\n",
      "Found the word pussy in the dictionary\n",
      "Found the word sad in the dictionary\n",
      "Found the word hell in the dictionary\n"
     ]
    }
   ],
   "source": [
    "profanity_words = ['fuck', 'pussy', 'sad', 'hell']\n",
    "for w in profanity_words:\n",
    "    if w in emb_dict.keys():\n",
    "        print('Found the word {} in the dictionary'.format(w))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIM = 25\n",
    "emb_matrix = np.zeros((NB_WORDS, GLOVE_DIM))\n",
    "\n",
    "for w, i in tk.word_index.items():\n",
    "    # The word_index contains a token for all words of the training data so we need to limit that\n",
    "    if i < NB_WORDS:\n",
    "        vect = emb_dict.get(w)\n",
    "        # Check if the word from the training data occurs in the GloVe word embeddings\n",
    "        # Otherwise the vector is kept with only zeros\n",
    "        if vect is not None:\n",
    "            emb_matrix[i] = vect\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dell-3060/.local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/dell-3060/.local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 24, 25)            250000    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 20)                3680      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 253,722\n",
      "Trainable params: 253,722\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM \n",
    "lstm_out = 20\n",
    "\n",
    "emb_model2 = models.Sequential()\n",
    "emb_model2.add(layers.Embedding(NB_WORDS, GLOVE_DIM, input_length=MAX_LEN))\n",
    "emb_model2.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
    "emb_model2.add(layers.Dense(2, activation='softmax'))\n",
    "emb_model2.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/dell-3060/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 129251 samples, validate on 14362 samples\n",
      "Epoch 1/20\n",
      "129251/129251 [==============================] - 6s 47us/step - loss: 0.2304 - acc: 0.9239 - val_loss: 0.1467 - val_acc: 0.9500\n",
      "Epoch 2/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1424 - acc: 0.9522 - val_loss: 0.1281 - val_acc: 0.9571\n",
      "Epoch 3/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1311 - acc: 0.9558 - val_loss: 0.1243 - val_acc: 0.9584\n",
      "Epoch 4/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1266 - acc: 0.9575 - val_loss: 0.1230 - val_acc: 0.9591\n",
      "Epoch 5/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1240 - acc: 0.9575 - val_loss: 0.1233 - val_acc: 0.9607\n",
      "Epoch 6/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1227 - acc: 0.9582 - val_loss: 0.1194 - val_acc: 0.9605\n",
      "Epoch 7/20\n",
      "129251/129251 [==============================] - 6s 44us/step - loss: 0.1213 - acc: 0.9586 - val_loss: 0.1188 - val_acc: 0.9610\n",
      "Epoch 8/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1197 - acc: 0.9584 - val_loss: 0.1179 - val_acc: 0.9615\n",
      "Epoch 9/20\n",
      "129251/129251 [==============================] - 6s 44us/step - loss: 0.1181 - acc: 0.9589 - val_loss: 0.1173 - val_acc: 0.9614\n",
      "Epoch 10/20\n",
      "129251/129251 [==============================] - 6s 44us/step - loss: 0.1162 - acc: 0.9594 - val_loss: 0.1186 - val_acc: 0.9614\n",
      "Epoch 11/20\n",
      "129251/129251 [==============================] - 6s 44us/step - loss: 0.1153 - acc: 0.9598 - val_loss: 0.1173 - val_acc: 0.9620\n",
      "Epoch 12/20\n",
      "129251/129251 [==============================] - 6s 45us/step - loss: 0.1142 - acc: 0.9595 - val_loss: 0.1169 - val_acc: 0.9614\n",
      "Epoch 13/20\n",
      "129251/129251 [==============================] - 6s 45us/step - loss: 0.1129 - acc: 0.9603 - val_loss: 0.1180 - val_acc: 0.9608\n",
      "Epoch 14/20\n",
      "129251/129251 [==============================] - 6s 44us/step - loss: 0.1117 - acc: 0.9602 - val_loss: 0.1169 - val_acc: 0.9614\n",
      "Epoch 15/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1103 - acc: 0.9607 - val_loss: 0.1190 - val_acc: 0.9621\n",
      "Epoch 16/20\n",
      "129251/129251 [==============================] - 6s 43us/step - loss: 0.1091 - acc: 0.9612 - val_loss: 0.1177 - val_acc: 0.9614\n",
      "Epoch 17/20\n",
      "129251/129251 [==============================] - 6s 44us/step - loss: 0.1083 - acc: 0.9612 - val_loss: 0.1202 - val_acc: 0.9612\n",
      "Epoch 18/20\n",
      "129251/129251 [==============================] - 6s 47us/step - loss: 0.1070 - acc: 0.9614 - val_loss: 0.1200 - val_acc: 0.9605\n",
      "Epoch 19/20\n",
      "129251/129251 [==============================] - 7s 53us/step - loss: 0.1062 - acc: 0.9615 - val_loss: 0.1198 - val_acc: 0.9603\n",
      "Epoch 20/20\n",
      "129251/129251 [==============================] - 6s 47us/step - loss: 0.1053 - acc: 0.9618 - val_loss: 0.1206 - val_acc: 0.9606\n"
     ]
    }
   ],
   "source": [
    "emb_history2 = deep_model(emb_model2, X_train_emb, y_train_emb, X_valid_emb, y_valid_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15957/15957 [==============================] - 1s 45us/step\n",
      "/n\n",
      "Test accuracy of word embedding model 2: 95.71%\n"
     ]
    }
   ],
   "source": [
    "emb_results2 = test_model(emb_model2, X_train_seq_trunc, y_train_oh, X_test_seq_trunc, y_test_oh, 3)\n",
    "print('/n')\n",
    "print('Test accuracy of word embedding model 2: {0:.2f}%'.format(emb_results2[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "    0   0   0   7 132   6]]\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "twt = [\"i fuck you\"]\n",
    "#vectorizing the tweet by the pre-fitted tokenizer instance\n",
    "twt = tk.texts_to_sequences(twt)\n",
    "#padding the tweet to have exactly the same shape as `embedding_2` input\n",
    "twt = pad_sequences(twt, maxlen=24, dtype='int32', value=0)\n",
    "print(twt)\n",
    "sentiment = emb_model2.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"positive\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"negative\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "senti: [0.00250683 0.99749315]\n",
      "negative\n"
     ]
    }
   ],
   "source": [
    "model = load_model('output/model/model.h5')\n",
    "sentiment = model.predict(twt,batch_size=1,verbose = 2)[0]\n",
    "print(\"senti:\",sentiment)\n",
    "if(np.argmax(sentiment) == 0):\n",
    "    print(\"positive\")\n",
    "elif (np.argmax(sentiment) == 1):\n",
    "    print(\"negative\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
